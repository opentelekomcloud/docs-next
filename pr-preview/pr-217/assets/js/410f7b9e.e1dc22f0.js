"use strict";(self.webpackChunkdocs_next=self.webpackChunkdocs_next||[]).push([[39757],{30421:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>r,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"blueprints/by-use-case/ai/deploying-nvidia-operator-on-cce","title":"Deploying Nvidia Operator on CCE","description":"Overview","source":"@site/docs/blueprints/by-use-case/ai/deploying-openwebui-on-a-cce-cluster-with-gpu-support copy.md","sourceDirName":"blueprints/by-use-case/ai","slug":"/blueprints/by-use-case/ai/deploying-nvidia-operator-on-cce","permalink":"/docs-next/pr-preview/pr-217/docs/blueprints/by-use-case/ai/deploying-nvidia-operator-on-cce","draft":false,"unlisted":false,"editUrl":"https://github.com/opentelekomcloud/docs-next/tree/main/docs/blueprints/by-use-case/ai/deploying-openwebui-on-a-cce-cluster-with-gpu-support copy.md","tags":[{"inline":true,"label":"nvidia","permalink":"/docs-next/pr-preview/pr-217/docs/tags/nvidia"},{"inline":true,"label":"nvidia-operator","permalink":"/docs-next/pr-preview/pr-217/docs/tags/nvidia-operator"},{"inline":true,"label":"gpu","permalink":"/docs-next/pr-preview/pr-217/docs/tags/gpu"},{"inline":true,"label":"ai","permalink":"/docs-next/pr-preview/pr-217/docs/tags/ai"}],"version":"current","frontMatter":{"id":"deploying-nvidia-operator-on-cce","title":"Deploying Nvidia Operator on CCE","tags":["nvidia","nvidia-operator","gpu","ai"]},"sidebar":"blueprintsSidebar","previous":{"title":"AI & LLMs","permalink":"/docs-next/pr-preview/pr-217/docs/blueprints/by-use-case/ai/"},"next":{"title":"Securely Expose Remote Ollama Endpoints to your Development Machine","permalink":"/docs-next/pr-preview/pr-217/docs/blueprints/by-use-case/ai/securely-expose-remote-ollama-endpoints-to-your-development-machine"}}');var t=i(74848),o=i(28453);const r={id:"deploying-nvidia-operator-on-cce",title:"Deploying Nvidia Operator on CCE",tags:["nvidia","nvidia-operator","gpu","ai"]},a="Deploying the NVIDIA Operator on CCE",l={},d=[{value:"Overview",id:"overview",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Step-by-Step Guide",id:"step-by-step-guide",level:2},{value:"1. <strong>Prepare GPU Nodes</strong>",id:"1-prepare-gpu-nodes",level:3},{value:"2. <strong>Install NVIDIA GPU Plugin</strong>",id:"2-install-nvidia-gpu-plugin",level:3},{value:"3. <strong>Deploy Nvidia Operator via Helm</strong>",id:"3-deploy-nvidia-operator-via-helm",level:3},{value:"4. Deploy an Application with GPU Support",id:"4-deploy-an-application-with-gpu-support",level:3},{value:"5. Test the Pod",id:"5-test-the-pod",level:3},{value:"Troubleshooting Tips",id:"troubleshooting-tips",level:2}];function c(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",mdxAdmonitionTitle:"mdxAdmonitionTitle",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"deploying-the-nvidia-operator-on-cce",children:"Deploying the NVIDIA Operator on CCE"})}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"The NVIDIA Operator is a critical tool for effectively managing GPU resources in Kubernetes clusters. It serves as an abstraction layer over Kubernetes APIs, automating tasks such as dynamic provisioning, driver updates, resource allocation, and optimization for GPU-intensive workloads, thereby simplifying the deployment and management of GPU-accelerated applications. Its functionality extends to dynamic provisioning of GPUs on demand, managing driver updates, optimizing resource allocation for varied workloads, and integrating with monitoring tools for comprehensive insights into GPU usage and health. This guide outlines how to deploy the NVIDIA Operator on CCE cluster. The process involves preparing GPU nodes, installing necessary components, configuring the cluster for GPU support, deploying an application leveraging GPUs, and verifying functionality."}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Access to CCE with kubectl."}),"\n",(0,t.jsx)(n.li,{children:"Helm installed on your system."}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"step-by-step-guide",children:"Step-by-Step Guide"}),"\n",(0,t.jsxs)(n.h3,{id:"1-prepare-gpu-nodes",children:["1. ",(0,t.jsx)(n.strong,{children:"Prepare GPU Nodes"})]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Go to the Open Telekom Cloud console and choose the specific cluster where you want to add the GPU node pool."}),"\n",(0,t.jsxs)(n.li,{children:["In the left sidebar select nodes and from there click on ",(0,t.jsx)(n.em,{children:"Create Node Pool"}),"."]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Configure the GPU Node Pool"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Name"}),": Assign a meaningful name to your GPU node pool, such as ",(0,t.jsx)(n.code,{children:"gpu-workers"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Flavor Selection"}),": Choose a flavor that includes GPU resources. Look for options like ",(0,t.jsx)(n.code,{children:"pi2.2xlarge"})," or similar GPU-accelerated instances available."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Annotations"}),": If required by your cluster's configuration, add any necessary annotations."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Taints or Tolerations"}),": Set taints or tolerations to manage pod scheduling. For GPU nodes, you might set a taint like ",(0,t.jsx)(n.code,{children:"nvidia.com/gpu=true:NoExecute"})," and ensure pods requiring GPUs have the appropriate toleration."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"image",src:i(85595).A+"",width:"1067",height:"826"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"After createing the node pool scale it to the desierd count."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Verify Nodes Joined"}),": Wait for some minutes so that the nodes get provisioned and then check if they have joined the cluster with:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'kubectl get nodes --show-labels | grep "nvidia"\n'})}),"\n",(0,t.jsxs)(n.p,{children:["joined nodes should contain a label with ",(0,t.jsx)(n.code,{children:"accelerator"})," key and ",(0,t.jsx)(n.code,{children:"nvidia*"})," as value (e.g.  ",(0,t.jsx)(n.strong,{children:"accelerator=nvidia-t4"}),")."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h3,{id:"2-install-nvidia-gpu-plugin",children:["2. ",(0,t.jsx)(n.strong,{children:"Install NVIDIA GPU Plugin"})]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Install the Plugin"}),": From sidebar select ",(0,t.jsx)(n.strong,{children:"Add-ons"})," and install the ",(0,t.jsx)(n.strong,{children:"CCE AI Suite (NVIDIA GPU)"}),".\n",(0,t.jsx)(n.img,{alt:"image",src:i(21156).A+"",width:"656",height:"498"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Configure the Plugin"}),": Configre the plugin. For more information see ",(0,t.jsx)(n.a,{href:"https://github.com/kubernetes-sigs/external-dnshttps://docs.otc.t-systems.com/cloud-container-engine/umn/add-ons/cloud_native_heterogeneous_computing_add-ons/cce_ai_suite_nvidia_gpu.html",children:"CCE AI Suite (NVIDIA GPU)"}),".\n",(0,t.jsx)(n.img,{alt:"image",src:i(54275).A+"",width:"895",height:"648"})]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.admonition,{type:"warning",children:[(0,t.jsx)(n.mdxAdmonitionTitle,{}),(0,t.jsxs)(n.p,{children:["Selected driver must be compatible with the gpu node and also supported by Nvidia Operator or else the cluster cannot allocate gpu resources.\nCheck supported drivers here ",(0,t.jsx)(n.a,{href:"https://github.com/kubernetes-sigs/external-dnshttps://docs.otc.t-systems.com/cloud-container-engine/umn/add-ons/cloud_native_heterogeneous_computing_add-ons/cce_ai_suite_nvidia_gpu.html",children:"CCE AI Suite (NVIDIA GPU)"}),", ",(0,t.jsx)(n.a,{href:"https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/platform-support.html",children:"Platform Support"}),"."]})]}),"\n",(0,t.jsxs)(n.h3,{id:"3-deploy-nvidia-operator-via-helm",children:["3. ",(0,t.jsx)(n.strong,{children:"Deploy Nvidia Operator via Helm"})]}),"\n",(0,t.jsxs)(n.p,{children:["Create a ",(0,t.jsx)(n.code,{children:"values.yaml"})," file to include the needed configurations:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"# values.yaml\n hostPaths:\n# default driver installation directory on OTC CCE is as below\ndriverInstallDir: \"/usr/local/nvidia/\"\n\n# driver installation is disabled because it's already installed via CCE AI Suite\ndriver:\n  enabled: false\n\n# container toolkit installation is disabled because it's already installed via CCE AI Suite\ntoolkit:\n  enabled: false\n\n"})}),"\n",(0,t.jsx)(n.p,{children:"Now deploy the operator via helm:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Add the nvidia operator helm repository\nhelm repo add nvidia https://helm.ngc.nvidia.com/nvidia\n\n# Update helm repositories\nhelm repo update\n\n# Install nvidia operator with configuration values\n helm install --wait gpu-operator \\\n  -n gpu-operator --create-namespace \\\n  nvidia/gpu-operator \\\n  -f values.yaml \\\n  --version=v24.9.2\n"})}),"\n",(0,t.jsx)(n.h3,{id:"4-deploy-an-application-with-gpu-support",children:"4. Deploy an Application with GPU Support"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Create a Pod Manifest"}),": For example, deploying a CUDA job."]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'apiVersion: v1\nkind: Pod\nmetadata:\n  name: cuda-vectoradd\nspec:\n  restartPolicy: OnFailure\n  containers:\n  - name: cuda-vectoradd\n    image: "nvcr.io/nvidia/k8s/cuda-sample:vectoradd-cuda11.7.1-ubuntu20.04"\n    resources:\n      limits:\n        nvidia.com/gpu: 1\n'})}),"\n",(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Apply the Manifest"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl apply -f cuda-example.yaml\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"5-test-the-pod",children:"5. Test the Pod"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Check Pod Status"}),": Ensure pods are running."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl get pods -n default\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Verify Logs"}),": Check logs for GPU activity."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl logs -f cuda-example-<pod-name> -n default\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"The logs should indicate that the operation was succesfull. Like as follow:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"[Vector addition of 50000 elements]\nCopy input data from the host memory to the CUDA device\nCUDA kernel launch with 196 blocks of 256 threads\nCopy output data from the CUDA device to the host memory\nTest PASSED\nDone\n"})}),"\n",(0,t.jsx)(n.h2,{id:"troubleshooting-tips",children:"Troubleshooting Tips"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Verify NVIDIA drivers are installed on nodes."}),"\n",(0,t.jsx)(n.li,{children:"Check resource requests and limits against cluster capacity."}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},54275:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/configure-plugin-1b6d956a809dad2efa39b93cfd533e86.png"},85595:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/create-node-pool-b1057ce3e16a4958f3b14db0b78d7f99.png"},21156:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/install-plugin-1f8a18536e91e328ef9c196b2a8cc4cd.png"},28453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>a});var s=i(96540);const t={},o=s.createContext(t);function r(e){const n=s.useContext(o);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);