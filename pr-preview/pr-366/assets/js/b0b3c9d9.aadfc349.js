"use strict";(self.webpackChunkdocs_next=self.webpackChunkdocs_next||[]).push([[99001],{60601:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>c,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"best-practices/containers/cloud-container-engine/auto-scaling-based-on-elb-monitoring-metrics/auto-scaling-based-on-elb-monitoring-metrics","title":"Auto Scaling Based on ELB Monitoring Metrics","description":"By default, Kubernetes scales a workload based on resource usage metrics such as CPU and memory. However, this mechanism cannot reflect the real-time resource usage when traffic bursts arrive, because the collected CPU and memory usage data lags behind the actual load balancer traffic metrics. For some services (such as flash sale and social media) that require fast auto scaling, scaling based on this rule may not be performed in a timely manner and cannot meet these services\' actual needs. In this case, auto scaling based on ELB QPS data can respond to service requirements more timely.","source":"@site/docs/best-practices/containers/cloud-container-engine/auto-scaling-based-on-elb-monitoring-metrics/auto-scaling-based-on-elb-monitoring-metrics.md","sourceDirName":"best-practices/containers/cloud-container-engine/auto-scaling-based-on-elb-monitoring-metrics","slug":"/best-practices/containers/cloud-container-engine/auto-scaling-based-on-elb-monitoring-metrics/","permalink":"/docs-next/pr-preview/pr-366/docs/best-practices/containers/cloud-container-engine/auto-scaling-based-on-elb-monitoring-metrics/","draft":false,"unlisted":false,"editUrl":"https://github.com/opentelekomcloud/docs-next/tree/main/docs/best-practices/containers/cloud-container-engine/auto-scaling-based-on-elb-monitoring-metrics/auto-scaling-based-on-elb-monitoring-metrics.md","tags":[{"inline":true,"label":"cce","permalink":"/docs-next/pr-preview/pr-366/docs/tags/cce"},{"inline":true,"label":"elb","permalink":"/docs-next/pr-preview/pr-366/docs/tags/elb"},{"inline":true,"label":"prometheus","permalink":"/docs-next/pr-preview/pr-366/docs/tags/prometheus"},{"inline":true,"label":"prometheus-exporter","permalink":"/docs-next/pr-preview/pr-366/docs/tags/prometheus-exporter"},{"inline":true,"label":"cloudeye","permalink":"/docs-next/pr-preview/pr-366/docs/tags/cloudeye"},{"inline":true,"label":"swr","permalink":"/docs-next/pr-preview/pr-366/docs/tags/swr"}],"version":"current","frontMatter":{"id":"auto-scaling-based-on-elb-monitoring-metrics","title":"Auto Scaling Based on ELB Monitoring Metrics","tags":["cce","elb","prometheus","prometheus-exporter","cloudeye","swr"]},"sidebar":"bestPracticesSidebar","previous":{"title":"Issue an ACME Certificate with DNS01 Solver in CCE","permalink":"/docs-next/pr-preview/pr-366/docs/best-practices/containers/cloud-container-engine/issue-an-acme-certificate-with-dns01-solver-in-cce"},"next":{"title":"Auto Scaling Based on ELB Monitoring Metrics with KEDA","permalink":"/docs-next/pr-preview/pr-366/docs/best-practices/containers/cloud-container-engine/auto-scaling-based-on-elb-monitoring-metrics/auto-scaling-based-on-elb-monitoring-metrics-with-keda"}}');var i=t(74848),r=t(28453);const o={id:"auto-scaling-based-on-elb-monitoring-metrics",title:"Auto Scaling Based on ELB Monitoring Metrics",tags:["cce","elb","prometheus","prometheus-exporter","cloudeye","swr"]},c="Auto Scaling Based on ELB Monitoring Metrics",a={},l=[{value:"Solution Design",id:"solution-design",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Building the Exporter Image",id:"building-the-exporter-image",level:2},{value:"Installing Buildpacks",id:"installing-buildpacks",level:3},{value:"Building the image",id:"building-the-image",level:3},{value:"Pushing the image to SWR",id:"pushing-the-image-to-swr",level:3},{value:"Installing Prometheus/Grafana Stack &amp; cloudeye-exporter artifacts",id:"installing-prometheusgrafana-stack--cloudeye-exporter-artifacts",level:2},{value:"Deploying the Exporter",id:"deploying-the-exporter",level:2},{value:"Getting ELB and ELB Listener IDs",id:"getting-elb-and-elb-listener-ids",level:2},{value:"Installing Nginx Ingress Controller",id:"installing-nginx-ingress-controller",level:2},{value:"Installing an Nginx Demo Workload",id:"installing-an-nginx-demo-workload",level:2},{value:"Appendix",id:"appendix",level:2},{value:"ELB Listener Metrics",id:"elb-listener-metrics",level:3},{value:"Developing an Exporter",id:"developing-an-exporter",level:3}];function d(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"auto-scaling-based-on-elb-monitoring-metrics",children:"Auto Scaling Based on ELB Monitoring Metrics"})}),"\n",(0,i.jsx)(n.p,{children:"By default, Kubernetes scales a workload based on resource usage metrics such as CPU and memory. However, this mechanism cannot reflect the real-time resource usage when traffic bursts arrive, because the collected CPU and memory usage data lags behind the actual load balancer traffic metrics. For some services (such as flash sale and social media) that require fast auto scaling, scaling based on this rule may not be performed in a timely manner and cannot meet these services' actual needs. In this case, auto scaling based on ELB QPS data can respond to service requirements more timely."}),"\n",(0,i.jsx)(n.h2,{id:"solution-design",children:"Solution Design"}),"\n",(0,i.jsxs)(n.p,{children:["This guide covers the environment setup required for implementing auto scaling based on ELB monitoring metrics. This setup is a prerequisite for both ",(0,i.jsx)(n.a,{href:"/docs-next/pr-preview/pr-366/docs/best-practices/containers/cloud-container-engine/auto-scaling-based-on-elb-monitoring-metrics/auto-scaling-based-on-elb-monitoring-metrics-with-keda",children:"KEDA"})," and ",(0,i.jsx)(n.a,{href:"/docs-next/pr-preview/pr-366/docs/best-practices/containers/cloud-container-engine/auto-scaling-based-on-elb-monitoring-metrics/auto-scaling-based-on-elb-monitoring-metrics-with-prometheus-adapter",children:"Prometheus Adapter"})," scaling solutions.The core of this solution is to collect ELB metric data and expose it to Prometheus for monitoring and scaling decisions. This is achieved by implementing a Prometheus exporter that retrieves the ELB metrics, transforms them into Prometheus-compatible format, and then publishes them so they can be scraped and stored by Prometheus."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Figure 1 ELB traffic flows and monitoring\ndata",src:t(89333).A+"",width:"1326",height:"789"})}),"\n",(0,i.jsxs)(n.p,{children:["In this guide, we\u2019ll use ",(0,i.jsx)(n.a,{href:"https://github.com/opentelekomcloud-blueprints/cloudeye-exporter",children:"cloudeye-exporter"}),", a custom Prometheus exporter designed to collect metrics from Open Telekom Cloud\u2019s Cloud Eye service. The exporter retrieves monitoring data, converts it into Prometheus-compatible format, and exposes it through an endpoint that Prometheus can scrape at regular intervals."]}),"\n",(0,i.jsx)(n.admonition,{type:"tip",children:(0,i.jsx)(n.p,{children:"Additional metrics can be gathered in the same manner by extending the exporter\u2019s configuration to include other service metrics. By adjusting the metric definitions and endpoints, the same workflow (data retrieval, conversion to Prometheus format, and periodic scraping) can be applied to monitor different resources such as ECS instances, EVS volumes, RDS databases, or networking components. This approach provides a consistent and scalable method for integrating a wide range of Open Telekom Cloud metrics into Prometheus."})}),"\n",(0,i.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"You should have a basic understanding of Prometheus and know how to develop a Prometheus exporter."}),"\n",(0,i.jsxs)(n.li,{children:["Ensure that the ",(0,i.jsx)(n.a,{href:"https://docs.otc.t-systems.com/cloud-container-engine/umn/add-ons/cloud_native_observability_add-ons/cloud_native_cluster_monitoring.html",children:"Cloud Native Cluster Monitoring"})," add-on is installed in your cluster, which must be running Kubernetes version 1.17 or later."]}),"\n",(0,i.jsxs)(n.li,{children:["Configure the Cloud Native Cluster Monitoring add-on to operate in ",(0,i.jsx)(n.code,{children:"server mode"}),"."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"building-the-exporter-image",children:"Building the Exporter Image"}),"\n",(0,i.jsx)(n.h3,{id:"installing-buildpacks",children:"Installing Buildpacks"}),"\n",(0,i.jsxs)(n.p,{children:["In this tutorial, the container image is built using ",(0,i.jsx)(n.a,{href:"https://buildpacks.io/",children:"Buildpacks"}),", a framework that automates the creation of production-ready container images without the need for a Dockerfile. Buildpacks detect the application\u2019s language and dependencies, compile the necessary runtime environment, and package everything into an optimized image. This approach simplifies image creation, ensures consistent builds across environments, and aligns with best practices for secure and maintainable containerized applications."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:'git clone https://github.com/opentelekomcloud-blueprints/cloudeye-exporter\ncurl -sSL "https://github.com/buildpacks/pack/releases/download/v0.32.1/pack-v0.32.1-linux.tgz" | sudo tar -C /usr/local/bin/ --no-same-owner -xzv pack\n'})}),"\n",(0,i.jsx)(n.admonition,{type:"note",children:(0,i.jsxs)(n.p,{children:["This method produces an OCI-compliant image, ensuring compatibility with nearly all container runtimes that support the Open Container Initiative standard. Alternatively, the same image can be built using a traditional Dockerfile and the ",(0,i.jsx)(n.code,{children:"docker build"})," command if preferred."]})}),"\n",(0,i.jsx)(n.h3,{id:"building-the-image",children:"Building the image"}),"\n",(0,i.jsxs)(n.p,{children:["The image name is ",(0,i.jsx)(n.code,{children:"cloudeye-exporter"})," and the image version is ",(0,i.jsx)(n.code,{children:"1.0"}),"."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"cd cloudeye-exporter\npack build cloudeye-exporter:v1 --builder gcr.io/buildpacks/builder:v1\n"})}),"\n",(0,i.jsx)(n.h3,{id:"pushing-the-image-to-swr",children:"Pushing the image to SWR"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["(Optional) Log in to the ",(0,i.jsx)(n.em,{children:"SWR console"}),", choose ",(0,i.jsx)(n.em,{children:"Organizations"}),"\nin the navigation pane, and click ",(0,i.jsx)(n.em,{children:"Create Organization"})," in the\nupper right corner of the page."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["In the navigation pane, choose ",(0,i.jsx)(n.em,{children:"My Images"})," and then click\n",(0,i.jsx)(n.em,{children:"Upload Through Client"}),". On the page displayed, click\n",(0,i.jsx)(n.em,{children:"Generate a temporary login command"})," and click\n",(0,i.jsx)(n.img,{alt:"image1",src:t(40915).A+"",width:"20",height:"21"})," to\ncopy the command."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:'Run the login command copied in the previous step on the cluster\nnode. If the login is successful, the message "Login\nSucceeded" is displayed.'}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Tag the ",(0,i.jsx)(n.code,{children:"cloudeye-exporter"})," image."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"docker tag {IMG1:TAG1}/{IMG_RERO_ADDR}/{ORG_NAME}/{IMG2:TAG2}\n"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"{IMG1:TAG1}"}),": name and tag of the local image to be uploaded."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"{IMG_RERO_ADDR}"}),": The domain name at the end of the login command in is the image repository address, which can be obtained on the SWR console."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"{ORG_NAME}"}),": name of the organization created in."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"{IMG2:TAG2}"}),": desired image name and tag to be displayed on the SWR console."]}),"\n"]}),"\n",(0,i.jsx)(n.admonition,{title:"Example",type:"note",children:(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"docker tag cloudeye-exporter:1.0 swr.eu-de.otc.t-systems.com/cloud-develop/cloudeye-exporter:1.0"})})}),"\n",(0,i.jsxs)(n.ol,{start:"5",children:["\n",(0,i.jsx)(n.li,{children:"Pushing the image to the image repository."}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"docker push {IMG_RERO_ADDR}/{ORG_NAME}/{IMG2:TAG2}\n"})}),"\n",(0,i.jsx)(n.admonition,{title:"Example",type:"note",children:(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"docker push swr.eu-de.otc.t-systems.com/cloud-develop/cloudeye-exporter:1.0"})})}),"\n",(0,i.jsx)(n.p,{children:"The following information will be returned upon a successful push:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"The push refers to repository [swr.eu-de.otc.t-systems.com/cloud-develop/cloudeye-exporter]\n030***: Pushed\nv1.0: digest: sha256:eb7e3bbd*** size: **\n"})}),"\n",(0,i.jsxs)(n.p,{children:["To view the pushed image, go to the SWR console and refresh the ",(0,i.jsx)(n.em,{children:"My Images"})," page."]}),"\n",(0,i.jsx)(n.h2,{id:"installing-prometheusgrafana-stack--cloudeye-exporter-artifacts",children:"Installing Prometheus/Grafana Stack & cloudeye-exporter artifacts"}),"\n",(0,i.jsxs)(n.p,{children:["Deploy the Prometheus and Grafana stack using the ",(0,i.jsx)(n.strong,{children:"kube-prometheus-stack"})," Helm chart. The configuration values will be automatically generated at ",(0,i.jsx)(n.code,{children:"deploy/manifests/prometheus-stack/override.yaml"}),". You can compare this file with the default ",(0,i.jsx)(n.code,{children:"default.yaml"})," to review the applied customizations."]}),"\n",(0,i.jsxs)(n.p,{children:["Execute ",(0,i.jsx)(n.code,{children:"./install-stack.sh"})," to begin the deployment. This script not only installs the kube-prometheus-stack but also sets up all required artifacts for the ",(0,i.jsx)(n.strong,{children:"cloudeye-exporter"})," integration."]}),"\n",(0,i.jsxs)(n.p,{children:["Run ",(0,i.jsx)(n.code,{children:"./install-stack.sh"}),". This script will deploy, besides the\n",(0,i.jsx)(n.strong,{children:"kube-prometheus-stack"}),", all the ",(0,i.jsx)(n.strong,{children:"cloudeye-exporter"})," related artifacts."]}),"\n",(0,i.jsx)(n.h2,{id:"deploying-the-exporter",children:"Deploying the Exporter"}),"\n",(0,i.jsxs)(n.p,{children:["Prometheus can dynamically monitor pods if you add Prometheus\nannotations to the pods (the default path is ",(0,i.jsx)(n.code,{children:"/metrics"}),"). Common annotations in Prometheus are as follows:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"prometheus.io/scrape"}),": If the value is ",(0,i.jsx)(n.code,{children:"true"}),", the pod will bemonitored."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"prometheus.io/path"}),": URL from which the data is collected. The default value is ",(0,i.jsx)(n.code,{children:"/metrics"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"prometheus.io/port"}),": port number of the endpoint to collect data from."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"prometheus.io/scheme"}),": Defaults to ",(0,i.jsx)(n.code,{children:"http"}),". If HTTPS is configured for security purposes, change the value to ",(0,i.jsx)(n.code,{children:"https"}),"."]}),"\n"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Use kubectl to connect to the cluster."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Create a secret, which will be used by ",(0,i.jsx)(n.strong,{children:"cloudeye-exporter"})," for\nauthentication."]}),"\n",(0,i.jsxs)(n.p,{children:["a.  Create a copy of clouds.tpl template, name it ",(0,i.jsx)(n.strong,{children:"clouds.yml"}),"\nfile with the following content:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"auth_url"}),": indicates the IAM endpoint, which can be obtained from ",(0,i.jsx)(n.a,{href:"https://docs.otc.t-systems.com/en-us/endpoint/index.html",children:"Regions and Endpoints"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"project_name"}),": indicates the project name. On the ",(0,i.jsx)(n.em,{children:"My Credential"})," page, view the project name and project ID in the ",(0,i.jsx)(n.em,{children:"Projects"})," area."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"access_key"})," and ",(0,i.jsx)(n.code,{children:"secret_key"}),": You can obtain them from ",(0,i.jsx)(n.em,{children:"Access Keys"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"region"}),": indicates the region name, which must correspond to the project in ",(0,i.jsx)(n.code,{children:"project_name"}),"."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"b.  Then encode it to base64 with the following command:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"base64 -i clouds.yaml \n"})}),"\n",(0,i.jsxs)(n.p,{children:["c.  Create the ",(0,i.jsx)(n.strong,{children:"cloudeye-exporter-clouds-secret.yaml"})," file with\nthe following content:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",metastring:'title="cloudeye-exporter-clouds-secret.yaml"',children:"apiVersion: v1\nkind: Secret\nmetadata:\n  name: cloudeye-exporter-clouds\n  namespace: default\ntype: Opaque\ndata:\n  clouds.yaml: Z2xvYmFsOg************************************************************\n"})}),"\n",(0,i.jsx)(n.p,{children:"d.  Create secret and deploy the exporter."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"kubectl apply -f cloudeye-exporter-clouds-secret.yaml\nkubectl apply -f cloudeye-exporter-clouds-secret.yaml\nkubectl apply -f cloudeye-exporter-clouds-secret.yaml\n"})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"getting-elb-and-elb-listener-ids",children:"Getting ELB and ELB Listener IDs"}),"\n",(0,i.jsxs)(n.p,{children:["In this example, the ELB metrics associated with the workload need to be\nmonitored. Therefore, the target workload must use the Service or\nIngress of the ",(0,i.jsx)(n.code,{children:"LoadBalancer"})," type."]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"View the access mode of the workload to be monitored and obtain the\nELB ID and ELB listener ID."}),"\n",(0,i.jsxs)(n.p,{children:["a.  On the CCE cluster console, choose ",(0,i.jsx)(n.em,{children:"Networking"}),". On the\n",(0,i.jsx)(n.em,{children:"Services"})," or ",(0,i.jsx)(n.em,{children:"Ingresses"})," tab page, view the Service or\nIngress of the ",(0,i.jsx)(n.code,{children:"LoadBalancer"})," type and click the load balancer\nto access the load balancer page and copy the ELB ID."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"image2",src:t(63178).A+"",width:"1182",height:"231"})}),"\n",(0,i.jsxs)(n.p,{children:["b.  On the ",(0,i.jsx)(n.em,{children:"Listeners"})," tab, view the listener corresponding to the\nworkload and copy the listener ID which corresponds to port ",(0,i.jsx)(n.code,{children:"80"}),"."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"image3",src:t(56665).A+"",width:"2464",height:"640"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Export the Elastic Load Balancer's ID and listener ID as an env\nvariables"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'export ELB_ID="66872*****"\nexport ELB_LISTENER_ID="94424*****"\n'})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"installing-nginx-ingress-controller",children:"Installing Nginx Ingress Controller"}),"\n",(0,i.jsxs)(n.p,{children:["Next, we are going to install the Nginx Ingress Controller using the\nscript ",(0,i.jsx)(n.strong,{children:"./install-ingress.sh"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",metastring:'title="install-ingress.sh"',children:"envsubst < nginx-ingress-controller/override.tpl  nginx-ingress-controller/override.yaml\nsleep 15\n\nhelm upgrade --install -f nginx-ingress-controller/override.yaml --install ingress-nginx ingress-nginx \\\n--repo https://kubernetes.github.io/ingress-nginx --namespace ingress-nginx --create-namespace\n"})}),"\n",(0,i.jsx)(n.h2,{id:"installing-an-nginx-demo-workload",children:"Installing an Nginx Demo Workload"}),"\n",(0,i.jsxs)(n.p,{children:["We are going to need a workload to test ",(0,i.jsx)(n.strong,{children:"HPA"})," and the autoscaling via our\ncustom ",(0,i.jsx)(n.strong,{children:"CloudEye"})," derived metrics. For that matter we will deploy a dummy\nnginx deployment and service using the script ",(0,i.jsx)(n.strong,{children:"./install-workload.sh"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"\nkubectl create namespace applications\nkubectl apply -f deploy/manifests/nginx-deployment.yaml\nkubectl apply -f deploy/manifests/nginx-ingress.yaml\n\n"})}),"\n",(0,i.jsx)(n.h2,{id:"appendix",children:"Appendix"}),"\n",(0,i.jsx)(n.h3,{id:"elb-listener-metrics",children:"ELB Listener Metrics"}),"\n",(0,i.jsx)(n.p,{children:"The following table lists the Elastic Load Balancer Listener metrics that can be collected\nusing the method described in sections above."}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Metric"}),(0,i.jsx)(n.th,{children:"Name"}),(0,i.jsx)(n.th,{children:"Unit"}),(0,i.jsx)(n.th,{children:"Description"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"m1_cps"})}),(0,i.jsx)(n.td,{children:"Concurrent Connections"}),(0,i.jsx)(n.td,{children:"Count"}),(0,i.jsx)(n.td,{children:"Number of concurrent connections processed by a load balance"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"m1e_server_rps"})}),(0,i.jsx)(n.td,{children:"Reset Packets from Backend Servers"}),(0,i.jsx)(n.td,{children:"Count/Second"}),(0,i.jsx)(n.td,{children:"Number of reset packets sent from the backend server to clients. These reset packages are generated by the backend server and then forwarded by load balancers."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"m1f_lvs_rps"})}),(0,i.jsx)(n.td,{children:"Reset Packets from Load Balancers"}),(0,i.jsx)(n.td,{children:"Count/Second"}),(0,i.jsx)(n.td,{children:"Number of reset packets sent from load balancers."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"m21_client_rps"})}),(0,i.jsx)(n.td,{children:"Reset Packets from Clients"}),(0,i.jsx)(n.td,{children:"Count/Second"}),(0,i.jsx)(n.td,{children:"Number of reset packets sent from clients to the backend server. These reset packages are generated by the clients and then forwarded by load balancers."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"m22_in_bandwidth"})}),(0,i.jsx)(n.td,{children:"Inbound Bandwidth"}),(0,i.jsx)(n.td,{children:"bit/s"}),(0,i.jsx)(n.td,{children:"Inbound bandwidth of a load balancer."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"m23_out_bandwidth"})}),(0,i.jsx)(n.td,{children:"Outbound Bandwidth"}),(0,i.jsx)(n.td,{children:"bit/s"}),(0,i.jsx)(n.td,{children:"Outbound bandwidth of a load balancer."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"m2_act_conn"})}),(0,i.jsx)(n.td,{children:"Active Connections"}),(0,i.jsx)(n.td,{children:"Count"}),(0,i.jsx)(n.td,{children:"Number of current active connections."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"m3_inact_conn"})}),(0,i.jsx)(n.td,{children:"Inactive Connections"}),(0,i.jsx)(n.td,{children:"Count"}),(0,i.jsx)(n.td,{children:"Number of current inactive connections."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"m4_ncps"})}),(0,i.jsx)(n.td,{children:"New Connections"}),(0,i.jsx)(n.td,{children:"Count"}),(0,i.jsx)(n.td,{children:"Number of current new connections."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"m5_in_pps"})}),(0,i.jsx)(n.td,{children:"Incoming Packets"}),(0,i.jsx)(n.td,{children:"Count"}),(0,i.jsx)(n.td,{children:"Number of packets sent to a load balancer."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"m6_out_pps"})}),(0,i.jsx)(n.td,{children:"Outgoing Packets"}),(0,i.jsx)(n.td,{children:"Count"}),(0,i.jsx)(n.td,{children:"Number of packets sent from a load balancer."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"m7_in_Bps"})}),(0,i.jsx)(n.td,{children:"Inbound Rate"}),(0,i.jsx)(n.td,{children:"byte/s"}),(0,i.jsx)(n.td,{children:"Number of incoming bytes per second on a load balancer."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"m8_out_Bps"})}),(0,i.jsx)(n.td,{children:"Outbound Rate"}),(0,i.jsx)(n.td,{children:"byte/s"}),(0,i.jsx)(n.td,{children:"Number of outgoing bytes per second on a load balancer."})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"developing-an-exporter",children:"Developing an Exporter"}),"\n",(0,i.jsxs)(n.p,{children:["Prometheus periodically calls the ",(0,i.jsx)(n.code,{children:"/metrics"})," API of the exporter to obtain metric data. Applications only need to report monitoring data\nthrough ",(0,i.jsx)(n.code,{children:"/metrics"}),". You can select a Prometheus client in a desired language and integrate it into applications to implement the\n",(0,i.jsx)(n.code,{children:"/metrics"})," API. For details about the client, see ",(0,i.jsx)(n.a,{href:"https://prometheus.io/docs/instrumenting/clientlibs/",children:"Prometheus Client Libraries"}),". For\ndetails about how to write the exporter, see ",(0,i.jsx)(n.a,{href:"https://prometheus.io/docs/instrumenting/writing_exporters/",children:"Writing Exporters"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"The monitoring data must be in the format that Prometheus supports. Each\ndata record provides the ELB ID, listener ID, namespace where the\nService is located, Service name, and Service UID as labels, as shown in\nthe following figure."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"image5",src:t(51384).A+"",width:"1452",height:"227"})}),"\n",(0,i.jsx)(n.p,{children:"To obtain the preceding data, perform the following steps:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Obtain all Services."}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"annotations"})," field in the returned information contains the\nELB associated with the Service."]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"kubernetes.io/elb.id"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"kubernetes.io/elb.class"})}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Use APIs in Querying Listeners to get the listener ID based on the\nload balancer ID obtained in the previous step."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Obtain the ELB monitoring data."}),"\n",(0,i.jsx)(n.p,{children:"The ELB monitoring data is obtained using the CES APIs described in\nQuerying Monitoring Data in Batches. For details about ELB\nmonitoring metrics, see Monitoring Metrics. Example:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"m1_cps"}),": number of concurrent connections"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"m5_in_pps"}),": number of incoming data packets"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"m6_out_pps"}),": number of outgoing data packets"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"m7_in_Bps"}),": incoming rate"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"m8_out_Bps"}),": outgoing rate"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Aggregate data in the format that Prometheus supports and expose the\ndata through the ",(0,i.jsx)(n.code,{children:"/metrics"})," API."]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},89333:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/overal-dd43f758b43bc13d48f4fa834a07db7d.png"},40915:(e,n,t)=>{t.d(n,{A:()=>s});const s="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAVCAYAAABG1c6oAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAACqSURBVDhPY/jy9dt/auJRA6H447v/L16+wovffMSiD4ixGvhgZ9f/8vJyvLhr5yMMfSCM1cAPDy/9P3nyFHa8d97/JkIGPjk47X9TUxMReNX/Cw93/e8iZCDMi1VYDYHiKpBX5/0/Q7yBXf93PsRUAMMQNaMGYlFHuoFPD/2fBoykaQefYVVHuoFY5JAxLQ0kMh2iGYCOwQaSlFPQDEDHYAOpiQe7gd/+AwBIw9kI+PIQ4wAAAABJRU5ErkJggg=="},56665:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/en-us_image_0000001380992506-4d5619a715514bf7525bb634554ee347.png"},51384:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/en-us_image_0000001381152106-c0afc526298384aa11a22dfa95af734e.png"},63178:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/en-us_image_0000001431432309-f6d88205377fb0bf73570be4219097ae.png"},28453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>c});var s=t(96540);const i={},r=s.createContext(i);function o(e){const n=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);